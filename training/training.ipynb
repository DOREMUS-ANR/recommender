{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load data in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pasquale/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/pasquale/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11815, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_folder = '../training_data/web-radio/output/rec'\n",
    "embDir = '../embeddings'\n",
    "what = 'artist'\n",
    "\n",
    "uri_file = '%s/%s.emb.u' % (embDir, what)\n",
    "vector_file = '%s/%s.emb.v' % (embDir, what)\n",
    "# header_file = '%s/%s.emb.h' % (embDir, what)\n",
    "training_file = '%s/%s.dat' % (training_data_folder, what)\n",
    "\n",
    "vectors = np.array([line.strip().split(' ') for line in codecs.open(vector_file, 'r', 'utf-8')])\n",
    "# heads = np.array([line.strip() for line in codecs.open(header_file, 'r', 'utf-8')])\n",
    "uris = np.array([line.strip() for line in codecs.open(uri_file, 'r', 'utf-8')])\n",
    "\n",
    "train_array = np.array([line.strip().split(' ') for line in codecs.open(training_file, 'r', 'utf-8')])\n",
    "train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embs(x):\n",
    "    v = vectors[np.argwhere(uris == x)]\n",
    "    if v.size == 0:\n",
    "        result = -2. * np.ones(vectors[0].size)\n",
    "    else:\n",
    "        result = v[0][0]\n",
    "    return result.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pasquale/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "col1 = np.array([get_embs(xi) for xi in train_array[:, 0]])\n",
    "col2 = np.array([get_embs(xi) for xi in train_array[:, 1]])\n",
    "col3 = np.array(train_array[:, 2]).astype('float32')\n",
    "col3 = col3.reshape((col3.size, 1))\n",
    "\n",
    "training_vector = np.concatenate((col1, col2, col3), axis=1)\n",
    "\n",
    "train, test = train_test_split(training_vector, train_size=0.3)\n",
    "\n",
    "train_vector = train[:, :-1]\n",
    "train_label = train[:, -1]\n",
    "\n",
    "test_vector = test[:, :-1]\n",
    "test_label = test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11815, 14)\n",
      "(11815, 14)\n",
      "(11815, 1)\n",
      "(11815, 29)\n",
      "(8271, 29)\n",
      "(3544, 28)\n",
      "(3544,)\n"
     ]
    }
   ],
   "source": [
    "print(col1.shape)\n",
    "print(col2.shape)\n",
    "print(col3.shape)\n",
    "\n",
    "print(training_vector.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print(train_vector.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 10\n",
    "batch_size = 1\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256  # 1st layer number of neurons\n",
    "n_hidden_2 = 256  # 2nd layer number of neurons\n",
    "num_input = train_vector[0].size\n",
    "num_output = int(num_input / 2)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_output]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l2(a, b, w):\n",
    "    # https://stackoverflow.com/a/8861999/1218213\n",
    "    q = tf.subtract(a, b)\n",
    "    # return np.sqrt((w * q * q).sum())\n",
    "    pow_q = tf.cast(tf.pow(q, 2), tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(tf.multiply(w, pow_q), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePenalty(expected, taken, total):\n",
    "    _exp = tf.size(expected)\n",
    "    _taken = tf.size(taken)\n",
    "    _tot = tf.size(total)\n",
    "    \n",
    "    return tf.divide(tf.subtract(_exp, _taken), _tot) \n",
    "    \n",
    "def neural_net_wrap(x, previous_out):\n",
    "    seed, target = tf.split(x, previous_out.shape, 1)\n",
    "    bs = tf.where(tf.equal(seed, -1))\n",
    "    bt = tf.where(tf.equal(target, -1))\n",
    "\n",
    "    _ones = np.ones(previous_out.shape)\n",
    "    max_distance = weighted_l2(_ones, _ones * -1, previous_out)\n",
    "    \n",
    "    bad_pos = tf.logical_or(bs, bt)\n",
    "    good_pos = tf.logical_not(bad_pos)\n",
    "    \n",
    "    _seed = tf.gather(seed, good_pos, axis=1)\n",
    "    _target = tf.gather(target, good_pos, axis=1)\n",
    "    _w = tf.gather(previous_out, good_pos, axis=1)\n",
    "\n",
    "    # distance\n",
    "    d = weightedL2(_seed, _target, _w)\n",
    "    # how much info I am not finding\n",
    "    lt = previous_out.shape[0]\n",
    "    penalty = tf.constant([computePenalty(bs[i], _seed[i], seed[i]) for i in range(0, lt)])\n",
    "    multiplier = tf.subtract(1., penalty)\n",
    "\n",
    "    # score\n",
    "    s = tf.divide(tf.subtract(max_distance, d), max_distance)\n",
    "    return tf.multiply(s, multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_loss(_sentinel=None, labels=None, logits=None, dim=-1, name=None):\n",
    "    l = col1[0].size\n",
    "    a = train_vector[:, 0:l]\n",
    "    b = train_vector[:, l:]\n",
    "\n",
    "    max_distance = weightedL2(np.ones(l), tf.constant(np.ones(l) * -1), logits)\n",
    "\n",
    "    predicted = compute_sim(a, b, logits, max_distance)\n",
    "    expected = tf.constant(train_label)\n",
    "\n",
    "    loss = tf.subtract(expected, predicted)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def weightedL2(a, b, w=1):\n",
    "    # https://stackoverflow.com/a/8861999/1218213\n",
    "    q = tf.subtract(a, b)\n",
    "    # return np.sqrt((w * q * q).sum())\n",
    "    pow_q = tf.cast(tf.pow(q, 2), tf.float32)\n",
    "    _w = tf.reshape(w, [w.shape[1]])\n",
    "\n",
    "    _sum = tf.reduce_sum(tf.multiply(_w, pow_q), axis=0, keepdims=True)\n",
    "    return tf.reshape(_sum, [1, 1])\n",
    "\n",
    "\n",
    "def compute_sim(seed, target, w, max_distance):\n",
    "    lt = seed.shape[0]\n",
    "    cost = [compute_similarity(seed[i], target[i], w, max_distance) for i in range(0, lt)]\n",
    "    return tf.convert_to_tensor(cost, tf.float32)\n",
    "\n",
    "\n",
    "def compute_similarity(seed, target, w, max_distance):\n",
    "    b1 = np.argwhere(seed >= -1)\n",
    "    b1.reshape(b1.size)\n",
    "    b2 = np.argwhere(target >= -1)\n",
    "    b2.reshape(b2.size)\n",
    "\n",
    "    good_pos = np.intersect1d(b1, b2)\n",
    "    if len(good_pos) == 0:\n",
    "        return tf.constant(0.0, shape=[1, 1])\n",
    "\n",
    "    _seed = seed[good_pos]\n",
    "    _target = target[good_pos]\n",
    "    _w = tf.gather(w, good_pos, axis=1)\n",
    "\n",
    "    # distance\n",
    "    d = weightedL2(_seed, _target, _w)\n",
    "    # how much info I am not finding\n",
    "    penalty = (len(b1) - len(good_pos)) / len(seed)\n",
    "    multiplier = 1. - penalty\n",
    "\n",
    "    # score\n",
    "    s = tf.divide(tf.subtract(max_distance, d), max_distance)\n",
    "    return tf.multiply(s, multiplier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = similarity_loss(logits=logits, labels=Y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = labels[idx]\n",
    "    return data_shuffle, labels_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3544, 14)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_labels = np.ones((train_vector.shape[0], num_output), dtype='float32')\n",
    "fake_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning\n",
      "Step 1, Minibatch Loss= 0.0296, Training Accuracy= 0.000\n",
      "Logits [[ -8422.724   -8038.133  -13766.626  -10481.018  -13289.251    -264.4525\n",
      "  -13350.081  -13181.779   -9711.509  -10764.15   -13100.075   -9080.267\n",
      "  -14499.189  -10063.788 ]]\n",
      "Step 2, Minibatch Loss= 0.0296, Training Accuracy= 0.000\n",
      "Logits [[-13865.452  -12249.402  -20640.875  -15956.003  -22288.156   -1161.3264\n",
      "  -22838.297  -21368.213  -14981.688  -18099.463  -21119.902  -15647.666\n",
      "  -23888.71   -17769.408 ]]\n",
      "Step 3, Minibatch Loss= 0.0297, Training Accuracy= 0.000\n",
      "Logits [[-10011.685  -12006.811  -15539.8545 -11470.255  -13581.098   -1106.6776\n",
      "  -15331.498  -14627.055  -13012.344  -12728.488  -14908.474   -9154.139\n",
      "  -14804.317  -14791.438 ]]\n",
      "Step 4, Minibatch Loss= 0.0296, Training Accuracy= 0.000\n",
      "Logits [[ -9302.604   -6255.8975 -12462.047   -9503.339  -16082.181    -250.7453\n",
      "  -14709.699  -14444.287   -8670.249  -14341.051  -13766.164  -11317.983\n",
      "  -16298.65   -12465.862 ]]\n",
      "Step 5, Minibatch Loss= 0.0296, Training Accuracy= 0.000\n",
      "Logits [[-20413.957  -21281.41   -28844.197  -22624.623  -28847.61     1494.1747\n",
      "  -33788.945  -31837.682  -23749.873  -26480.512  -30224.58   -19841.334\n",
      "  -31205.105  -27894.162 ]]\n",
      "Step 6, Minibatch Loss= 0.0296, Training Accuracy= 0.000\n",
      "Logits [[-25866.852 -21919.883 -36663.773 -27428.135 -41724.105   1444.138\n",
      "  -44033.273 -40716.992 -26560.045 -34900.137 -40795.668 -28942.035\n",
      "  -44770.934 -36465.14 ]]\n",
      "Step 7, Minibatch Loss= 0.0296, Training Accuracy= 0.000\n",
      "Logits [[-24548.482 -22918.814 -34143.176 -24928.168 -37345.625   2993.64\n",
      "  -41897.234 -38325.44  -26076.277 -32549.363 -37040.81  -24344.736\n",
      "  -40716.79  -34192.875]]\n",
      "Step 8, Minibatch Loss= 0.0295, Training Accuracy= 0.000\n",
      "Logits [[-17987.713 -14836.044 -24772.314 -17435.549 -27043.621   4664.987\n",
      "  -30017.66  -27605.479 -19071.322 -25252.91  -27220.328 -18412.008\n",
      "  -29953.81  -27071.559]]\n",
      "Step 9, Minibatch Loss= 0.0295, Training Accuracy= 0.000\n",
      "Logits [[-27375.104 -25326.256 -39916.28  -29890.092 -47632.72    8772.396\n",
      "  -51440.54  -46259.703 -30666.904 -39817.71  -44905.656 -31143.053\n",
      "  -50432.82  -41289.176]]\n",
      "Step 10, Minibatch Loss= 0.0294, Training Accuracy= 0.000\n",
      "Logits [[-20026.75  -16452.922 -30224.11  -22728.32  -36905.203   8205.501\n",
      "  -38972.28  -34015.355 -21969.027 -31398.537 -33277.055 -26717.277\n",
      "  -39760.227 -31911.92 ]]\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    print(\"Start learning\")\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y = next_batch(batch_size, train_vector, fake_labels)\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            lgs, loss, acc = sess.run([logits, loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "            print(\"Logits %s\" % lgs)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "#     print(\"Testing Accuracy:\",\n",
    "#           sess.run(accuracy, feed_dict={X: test_vector, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
